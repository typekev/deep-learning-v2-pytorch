{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(60),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomVerticalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "image_path = 'data'\n",
    "\n",
    "train_path = os.path.join(image_path, 'train')\n",
    "val_path = os.path.join(image_path, 'valid')\n",
    "test_path = os.path.join(image_path, 'test')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_path, train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_path, train_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_path, test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "loaders = {'train': train_loader, 'valid': val_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0 16.0 16.0 4.0 768.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_w_conv_out(conv, pool_stride = 1):\n",
    "    return (((conv[\"W\"] - conv[\"F\"] + (2*conv[\"P\"])) / conv[\"S\"]) + 1) / pool_stride\n",
    "\n",
    "conv1_w_in = 224\n",
    "conv1 = {\"W\": conv1_w_in, \"D\": 3, \"K\": 16, \"F\": 7, \"P\": 0, \"S\": 7}\n",
    "conv1_w_out = calc_w_conv_out(conv1)\n",
    "\n",
    "conv2 = {\"W\": conv1_w_out, \"D\": conv1[\"K\"], \"K\": 24, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv2_w_out = calc_w_conv_out(conv2, 2)\n",
    "\n",
    "conv3 = {\"W\": conv2_w_out, \"D\": conv2[\"K\"], \"K\": 32, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv3_w_out = calc_w_conv_out(conv3)\n",
    "\n",
    "conv4 = {\"W\": conv3_w_out, \"D\": conv3[\"K\"], \"K\": 48, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv4_w_out = calc_w_conv_out(conv4, 4)\n",
    "\n",
    "conv5 = {\"W\": conv4_w_out, \"D\": conv4[\"K\"], \"K\": 56, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv5_w_out = calc_w_conv_out(conv5)\n",
    "\n",
    "conv6 = {\"W\": conv5_w_out, \"D\": conv5[\"K\"], \"K\": 64, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv6_w_out = calc_w_conv_out(conv6, 4)\n",
    "\n",
    "conv7 = {\"W\": conv6_w_out, \"D\": conv6[\"K\"], \"K\": 176, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv7_w_out = calc_w_conv_out(conv7)\n",
    "\n",
    "conv8 = {\"W\": conv7_w_out, \"D\": conv7[\"K\"], \"K\": 192, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv8_w_out = calc_w_conv_out(conv8, 2)\n",
    "\n",
    "conv9 = {\"W\": conv8_w_out, \"D\": conv8[\"K\"], \"K\": 208, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv9_w_out = calc_w_conv_out(conv9)\n",
    "\n",
    "conv10 = {\"W\": conv9_w_out, \"D\": conv9[\"K\"], \"K\": 224, \"F\": 3, \"P\": 1, \"S\": 1}\n",
    "conv10_w_out = calc_w_conv_out(conv10, 2)\n",
    "\n",
    "\n",
    "conv_features_out = conv4_w_out**2 * conv4[\"K\"]\n",
    "\n",
    "#print(conv1_w_out, conv2_w_out, conv3_w_out, conv4_w_out, conv5_w_out, \n",
    "#      conv6_w_out, conv7_w_out, conv8_w_out, conv9_w_out, conv10_w_out, conv_features_out)\n",
    "\n",
    "print(conv1_w_out, conv2_w_out, conv3_w_out, conv4_w_out, conv_features_out)\n",
    "\n",
    "def make_nn_conv(conv):\n",
    "    return nn.Conv2d(conv[\"D\"], conv[\"K\"], conv[\"F\"], padding=conv[\"P\"], stride=conv[\"S\"])\n",
    "\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        ## Layer 1\n",
    "        self.conv1 = make_nn_conv(conv1)\n",
    "        self.conv2 = make_nn_conv(conv2)\n",
    "        ## Layer 2\n",
    "        self.conv3 = make_nn_conv(conv3)\n",
    "        self.conv4 = make_nn_conv(conv4)\n",
    "        ## Layer 3\n",
    "        #self.conv5 = make_nn_conv(conv5)\n",
    "        #self.conv6 = make_nn_conv(conv6)\n",
    "        ## Layer 4\n",
    "        #self.conv7 = make_nn_conv(conv7)\n",
    "        #self.conv8 = make_nn_conv(conv8)\n",
    "        ## Layer 5\n",
    "        #self.conv9 = make_nn_conv(conv9)\n",
    "        #self.conv10 = make_nn_conv(conv10)\n",
    "        \n",
    "        ## Layer 6\n",
    "        self.fc1 = nn.Linear(int(conv_features_out), 133)\n",
    "        ## Layer 7\n",
    "        #self.fc2 = nn.Linear(4096, 256)\n",
    "        ## Layer 8\n",
    "        #self.fc3 = nn.Linear(256, 133)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        batch_size = x.size()[0]\n",
    "\n",
    "        # layer 1        \n",
    "        x = F.dropout(F.relu(self.conv1(x)), 0.2)\n",
    "        x = F.dropout(F.max_pool2d(F.relu(self.conv2(x)), 2, 2), 0.2)\n",
    "        # layer 2\n",
    "        x = F.dropout(F.relu(self.conv3(x)), 0.2)\n",
    "        x = F.dropout(F.max_pool2d(F.relu(self.conv4(x)), 4, 4), 0.2)\n",
    "        # layer 3\n",
    "        #x = F.dropout(F.relu(self.conv5(x)), 0.2)\n",
    "        #x = F.dropout(F.max_pool2d(F.relu(self.conv6(x)), 4, 4), 0.2)\n",
    "        # layer 4\n",
    "        #x = F.dropout(F.relu(self.conv7(x)), 0.2)\n",
    "        #x = F.dropout(F.max_pool2d(F.relu(self.conv8(x)), 2, 2), 0.2)\n",
    "        # layer 5\n",
    "        #x = F.dropout(F.relu(self.conv9(x)), 0.2)\n",
    "        #x = F.dropout(F.max_pool2d(F.relu(self.conv10(x)), 2, 2), 0.2)\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        #x = F.dropout(F.relu(self.fc1(x)), 0.2)\n",
    "        #x = F.dropout(F.relu(self.fc2(x)), 0.2)\n",
    "        #x = F.log_softmax(self.fc3(x))\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        train_loss = train_loss/len(loaders['train'].sampler)\n",
    "\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        valid_loss = valid_loss/len(loaders['valid'].sampler)\n",
    "        \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print(f'Saved model.pt, validation decresed: {valid_loss_min} => {valid_loss}')\n",
    "            valid_loss_min = valid_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.983252 \tValidation Loss: 1.627381\n",
      "Saved model.pt, validation decresed: inf => 1.6273811308542887\n",
      "Epoch: 2 \tTraining Loss: 0.969191 \tValidation Loss: 1.246867\n",
      "Saved model.pt, validation decresed: 1.6273811308542887 => 1.2468668762842814\n",
      "Epoch: 3 \tTraining Loss: 0.841281 \tValidation Loss: 1.105946\n",
      "Saved model.pt, validation decresed: 1.2468668762842814 => 1.1059458653132122\n",
      "Epoch: 4 \tTraining Loss: 0.826974 \tValidation Loss: 1.021125\n",
      "Saved model.pt, validation decresed: 1.1059458653132122 => 1.0211247221628825\n",
      "Epoch: 5 \tTraining Loss: 0.840992 \tValidation Loss: 1.156472\n",
      "Epoch: 6 \tTraining Loss: 0.821332 \tValidation Loss: 1.071794\n",
      "Epoch: 7 \tTraining Loss: 0.821912 \tValidation Loss: 1.040645\n",
      "Epoch: 8 \tTraining Loss: 0.824944 \tValidation Loss: 1.078237\n",
      "Epoch: 9 \tTraining Loss: 0.825027 \tValidation Loss: 0.995162\n",
      "Saved model.pt, validation decresed: 1.0211247221628825 => 0.9951615842183431\n",
      "Epoch: 10 \tTraining Loss: 0.809538 \tValidation Loss: 1.029245\n",
      "Epoch: 11 \tTraining Loss: 0.806930 \tValidation Loss: 1.081986\n",
      "Epoch: 12 \tTraining Loss: 0.807031 \tValidation Loss: 0.994880\n",
      "Saved model.pt, validation decresed: 0.9951615842183431 => 0.9948795207341512\n",
      "Epoch: 13 \tTraining Loss: 0.805343 \tValidation Loss: 0.988689\n",
      "Saved model.pt, validation decresed: 0.9948795207341512 => 0.988689370950063\n",
      "Epoch: 14 \tTraining Loss: 0.817839 \tValidation Loss: 1.037652\n",
      "Epoch: 15 \tTraining Loss: 0.809634 \tValidation Loss: 0.969131\n",
      "Saved model.pt, validation decresed: 0.988689370950063 => 0.9691307322184245\n",
      "Epoch: 16 \tTraining Loss: 0.810503 \tValidation Loss: 1.067975\n",
      "Epoch: 17 \tTraining Loss: 0.810524 \tValidation Loss: 1.000474\n",
      "Epoch: 18 \tTraining Loss: 0.797834 \tValidation Loss: 1.020171\n",
      "Epoch: 19 \tTraining Loss: 0.789881 \tValidation Loss: 0.962119\n",
      "Saved model.pt, validation decresed: 0.9691307322184245 => 0.962119410832723\n",
      "Epoch: 20 \tTraining Loss: 0.799871 \tValidation Loss: 1.008159\n",
      "Epoch: 21 \tTraining Loss: 0.787023 \tValidation Loss: 0.946007\n",
      "Saved model.pt, validation decresed: 0.962119410832723 => 0.9460067105293274\n",
      "Epoch: 22 \tTraining Loss: 0.793335 \tValidation Loss: 0.984381\n",
      "Epoch: 23 \tTraining Loss: 0.778037 \tValidation Loss: 1.003446\n",
      "Epoch: 24 \tTraining Loss: 0.801062 \tValidation Loss: 1.004393\n",
      "Epoch: 25 \tTraining Loss: 0.790464 \tValidation Loss: 0.962585\n",
      "Epoch: 26 \tTraining Loss: 0.798115 \tValidation Loss: 0.948092\n",
      "Epoch: 27 \tTraining Loss: 0.785383 \tValidation Loss: 0.946518\n",
      "Epoch: 28 \tTraining Loss: 0.773952 \tValidation Loss: 0.973665\n",
      "Epoch: 29 \tTraining Loss: 0.766576 \tValidation Loss: 0.975218\n",
      "Epoch: 30 \tTraining Loss: 0.782362 \tValidation Loss: 0.964864\n",
      "Epoch: 31 \tTraining Loss: 0.764206 \tValidation Loss: 0.917221\n",
      "Saved model.pt, validation decresed: 0.9460067105293274 => 0.9172207363446554\n",
      "Epoch: 32 \tTraining Loss: 0.775355 \tValidation Loss: 0.940414\n",
      "Epoch: 33 \tTraining Loss: 0.781945 \tValidation Loss: 0.955834\n",
      "Epoch: 34 \tTraining Loss: 0.763964 \tValidation Loss: 0.902347\n",
      "Saved model.pt, validation decresed: 0.9172207363446554 => 0.9023466030756633\n",
      "Epoch: 35 \tTraining Loss: 0.771842 \tValidation Loss: 0.954993\n",
      "Epoch: 36 \tTraining Loss: 0.763053 \tValidation Loss: 0.948769\n",
      "Epoch: 37 \tTraining Loss: 0.761112 \tValidation Loss: 0.961761\n",
      "Epoch: 38 \tTraining Loss: 0.762432 \tValidation Loss: 0.953996\n",
      "Epoch: 39 \tTraining Loss: 0.773604 \tValidation Loss: 1.004865\n",
      "Epoch: 40 \tTraining Loss: 0.766008 \tValidation Loss: 0.988785\n",
      "Epoch: 41 \tTraining Loss: 0.790177 \tValidation Loss: 0.966322\n",
      "Epoch: 42 \tTraining Loss: 0.753316 \tValidation Loss: 0.889129\n",
      "Saved model.pt, validation decresed: 0.9023466030756633 => 0.8891289782524109\n",
      "Epoch: 43 \tTraining Loss: 0.763758 \tValidation Loss: 0.880837\n",
      "Saved model.pt, validation decresed: 0.8891289782524109 => 0.8808372847239176\n",
      "Epoch: 44 \tTraining Loss: 0.757041 \tValidation Loss: 0.937673\n",
      "Epoch: 45 \tTraining Loss: 0.766198 \tValidation Loss: 0.945621\n",
      "Epoch: 46 \tTraining Loss: 0.766976 \tValidation Loss: 0.940839\n",
      "Epoch: 47 \tTraining Loss: 0.761198 \tValidation Loss: 0.873177\n",
      "Saved model.pt, validation decresed: 0.8808372847239176 => 0.8731765977541606\n",
      "Epoch: 48 \tTraining Loss: 0.752289 \tValidation Loss: 0.999872\n",
      "Epoch: 49 \tTraining Loss: 0.750274 \tValidation Loss: 0.888908\n",
      "Epoch: 50 \tTraining Loss: 0.753992 \tValidation Loss: 0.954418\n"
     ]
    }
   ],
   "source": [
    "model = train(50, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.792176\n",
      "\n",
      "\n",
      "Test Accuracy: 66% (400/600)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
